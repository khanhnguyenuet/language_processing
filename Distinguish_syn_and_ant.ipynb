{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"W2V_150.txt\"\n",
    "antonym_path = \"Antonym_vietnamese.txt\"\n",
    "synonym_path = \"Synonym_vietnamese.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = open(path_text, encoding='utf8')\n",
    "antonym_file = open(antonym_path, encoding='utf8')\n",
    "synonym_file = open(synonym_path, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [line for line in w2v_file.readlines()]\n",
    "antonym_lines = [line for line in antonym_file.readlines()]\n",
    "synonym_lines = [line for line in synonym_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = words[0]\n",
    "vector_dimensions = words[1]\n",
    "words = words[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_words = []\n",
    "embedding = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, w in enumerate(words):\n",
    "    arr_words = words[idx].split(' ')\n",
    "    word = arr_words[0]\n",
    "    weight = []\n",
    "    for i in arr_words[2:len(arr_words)-1]:\n",
    "        weight.append(float(i))\n",
    "    embedding[word] = np.array(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_floats(start, end, size):\n",
    "    mid = start + (end-start)/2\n",
    "    x = mid + np.random.rand(size) * (end-mid)\n",
    "    sign = np.random.rand(size)\n",
    "    sign[sign < 0.5] = -1\n",
    "    sign[sign != -1] = 1\n",
    "    x = x*sign\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0.95241035,  1.72890687, -1.24102099, -1.606396  ,  1.45401335,\n",
       "        1.74104869, -0.37976942,  1.26168145,  0.34163943, -0.91179381,\n",
       "       -0.89672181, -0.34666418,  0.34734914,  1.47996769, -0.11181994,\n",
       "       -1.53517038, -0.48014476, -1.33112721,  0.11632293,  1.79555341,\n",
       "        1.48620722, -1.98800682, -0.01066458, -1.95380099,  0.29132906,\n",
       "       -0.20952752, -1.605953  ,  0.9330842 , -0.68669517,  0.83886817,\n",
       "        0.50857228, -0.972421  , -1.00065165, -1.44926315,  1.69431979,\n",
       "        1.75211712, -1.90137574,  1.29057611,  1.92342313,  0.11710734,\n",
       "        1.90146789,  0.39975585, -1.95560364,  0.56010451, -0.22647166,\n",
       "       -0.75814545,  1.75724926,  0.05925765, -0.63822265,  0.47498931,\n",
       "       -0.61094165,  1.62607219,  0.9432371 ,  0.66738467, -0.54383898,\n",
       "        0.72001594,  0.05673099, -0.24765063,  1.17573799, -0.56283261,\n",
       "        1.32452732,  0.80622269, -1.97938518, -1.27920556, -1.94086255,\n",
       "       -0.90026438, -1.04407522,  0.44949879,  0.39758785,  0.66319221,\n",
       "        0.46046611,  0.34987634, -1.00966428, -1.74358936, -0.2442432 ,\n",
       "        1.97740718,  1.0282015 ,  0.67216881, -1.62375773,  0.60073771,\n",
       "        1.41588299,  0.06667692,  1.62920533, -1.00259585, -0.35176493,\n",
       "       -1.64678017,  1.19837567,  0.87051297,  1.26958571,  0.01058725,\n",
       "       -0.9284998 , -0.53142846, -1.01362522,  1.83077021, -1.67463298,\n",
       "        1.14196506,  0.97772367,  0.1491062 ,  0.33910828, -0.21318636,\n",
       "        0.79996101,  1.53477834, -1.84539322, -1.04584881,  0.53530927,\n",
       "       -1.5148494 ,  1.71219916, -1.64116163,  0.05075137,  0.6977033 ,\n",
       "        1.23020247, -1.24756206, -1.86660825, -0.88325639, -1.16106498,\n",
       "       -1.55145675,  0.27693535,  1.63048517, -1.75513257, -1.9811517 ,\n",
       "        1.41760285, -1.5793025 , -0.84031657, -0.11070407, -1.24760751,\n",
       "        1.27555708, -1.71439678, -0.97267164,  1.86107645,  1.82117756,\n",
       "       -0.63317584,  1.97023803, -0.30301638,  1.13824867,  0.46120093,\n",
       "        0.22771952,  0.88901251,  0.43993996, -1.51095517,  0.83365464,\n",
       "        0.11257399,  0.40426694, -0.42451326,  1.87118409,  1.52834006,\n",
       "        0.57419451, -1.81049229,  0.48368225,  0.93991914, -1.52652098])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "generate_floats(-2, 2, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_word1 = []\n",
    "antonym_word2 = []\n",
    "synonym_word1 = []\n",
    "synonym_word2 = []\n",
    "\n",
    "for idx, w in enumerate(antonym_lines):\n",
    "    arr_words = antonym_lines[idx].split(' ')\n",
    "    antonym_word1.append(arr_words[0])\n",
    "    antonym_word2.append(arr_words[1].split('\\n')[0])\n",
    "\n",
    "for idx, w in enumerate(synonym_lines):\n",
    "    arr_words = synonym_lines[idx].split(' ')\n",
    "    synonym_word1.append(arr_words[0])\n",
    "    synonym_word2.append(arr_words[1].split('\\n')[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, random_state=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                     random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=20).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([])\n",
    "b = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(antonym_word1)):\n",
    "    if antonym_word1[i] not in embedding:\n",
    "        embedding[antonym_word1[i]] = generate_floats(-2, 2, 150)\n",
    "    if antonym_word2[i] not in embedding:\n",
    "        embedding[antonym_word2[i]] = generate_floats(-2, 2, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(antonym_word1)):\n",
    "    if len(a) == 0:\n",
    "        a = np.append(a, embedding[antonym_word1[i]])\n",
    "        b = np.append(b, embedding[antonym_word2[i]])\n",
    "    else:\n",
    "        a = np.vstack((a, embedding[antonym_word1[i]]))\n",
    "        b = np.vstack((b, embedding[antonym_word2[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(synonym_word1)):\n",
    "    if synonym_word1[i] not in embedding:\n",
    "        embedding[synonym_word1[i]] = generate_floats(-2, 2, 150)\n",
    "    if synonym_word2[i] not in embedding:\n",
    "        embedding[synonym_word2[i]] = generate_floats(-2, 2, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([])\n",
    "b1 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(synonym_word1)):\n",
    "    if len(a1) == 0:\n",
    "        a1 = np.append(a1, embedding[synonym_word1[i]])\n",
    "        b1 = np.append(b1, embedding[synonym_word2[i]])\n",
    "    else:\n",
    "        a1 = np.vstack((a1, embedding[synonym_word1[i]]))\n",
    "        b1 = np.vstack((b1, embedding[synonym_word2[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_syn = np.ones(len(a1))\n",
    "y_ant = np.zeros(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((y_syn, y_ant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_train = np.concatenate((a, b), axis=1)\n",
    "syn_train = np.concatenate((a1, b1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((syn_train, ant_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_path = \"ViData/ViCon/400_noun_pairs.txt\"\n",
    "adj_path = \"ViData/ViCon/600_adj_pairs.txt\"\n",
    "verb_path = \"ViData/ViCon/400_verb_pairs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_file = open(noun_path, encoding='utf8')\n",
    "adj_file = open(adj_path, encoding='utf8')\n",
    "verb_file = open(verb_path, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_line_test = [line for line in noun_file.readlines()]\n",
    "verb_line_test = [line for line in verb_file.readlines()]\n",
    "adj_line_test = [line for line in adj_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_line_test = noun_line_test[1:]\n",
    "verb_line_test = verb_line_test[1:]\n",
    "adj_line_test = adj_line_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test_1 = []\n",
    "words_test_2 = []\n",
    "label = []\n",
    "\n",
    "for idx, w in enumerate(noun_line_test):\n",
    "    words_test_1.append(noun_line_test[idx].split('\\t')[0])\n",
    "    words_test_2.append(noun_line_test[idx].split('\\t')[1])\n",
    "    if noun_line_test[idx].split('\\t')[2].split('\\n')[0] == 'ANT':\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "        \n",
    "for idx, w in enumerate(verb_line_test):\n",
    "    words_test_1.append(verb_line_test[idx].split('\\t')[0])\n",
    "    words_test_2.append(verb_line_test[idx].split('\\t')[1])\n",
    "    if verb_line_test[idx].split('\\t')[2].split('\\n')[0] == 'ANT':\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "        \n",
    "for idx, w in enumerate(adj_line_test):\n",
    "    words_test_1.append(adj_line_test[idx].split('\\t')[0])\n",
    "    words_test_2.append(adj_line_test[idx].split('\\t')[1])\n",
    "    if adj_line_test[idx].split('\\t')[2].split('\\n')[0] == 'ANT':\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test_vt1 = np.array([])\n",
    "words_test_vt2 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words_test_1)):\n",
    "    if words_test_1[i] not in embedding:\n",
    "        embedding[words_test_1[i]] = generate_floats(-2, 2, 150)\n",
    "    if words_test_2[i] not in embedding:\n",
    "        embedding[words_test_2[i]] = generate_floats(-2, 2, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words_test_1)):\n",
    "    if len(words_test_vt1) == 0:\n",
    "        words_test_vt1 = np.append(words_test_vt1, embedding[words_test_1[i]])\n",
    "        words_test_vt2 = np.append(words_test_vt2, embedding[words_test_2[i]])\n",
    "    else:\n",
    "        words_test_vt1 = np.vstack((words_test_vt1, embedding[words_test_1[i]]))\n",
    "        words_test_vt2 = np.vstack((words_test_vt2, embedding[words_test_2[i]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.concatenate((words_test_vt1, words_test_vt2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(random_state=0, multi_class='ovr').fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(random_state=0, learning_rate_init=0.0005).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_pred = mlp.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_pred = lg.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1400"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "len(label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_lg = 0\n",
    "c_mlp = 0\n",
    "for i in range(len(label_)):\n",
    "    if lg_pred[i] == label_[i]:\n",
    "        c_lg += 1\n",
    "    if mlp_pred[i] == label_[i]:\n",
    "        c_mlp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.65\n0.9992857142857143\n"
     ]
    }
   ],
   "source": [
    "print(c_lg/len(label_))\n",
    "print(c_mlp/len(label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "for i in range(len(lg_pred)):\n",
    "    if lg_pred[i] == label_[i]:\n",
    "        if lg_pred[i] == 1:\n",
    "            true_positive += 1\n",
    "    else:\n",
    "        if label_[i] == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            false_negative += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (true_positive) / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "F1 = 2 * ((precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.9371428571428572\n0.5952813067150635\n0.7280799112097669\n"
     ]
    }
   ],
   "source": [
    "print(precision)\n",
    "print(recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __test__(w1, w2):\n",
    "    x = np.concatenate((embedding[w1], embedding[w2]), axis=0)\n",
    "    x = x.reshape(1, 300)\n",
    "    return mlp.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "__test__('bao_vây', 'giải_vây')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}