{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_text = \"source/W2V_150.txt\"\n",
    "antonym_path = \"source/Antonym_vietnamese.txt\"\n",
    "synonym_path = \"source/Synonym_vietnamese.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = open(path_text, encoding='utf8')\n",
    "antonym_file = open(antonym_path, encoding='utf8')\n",
    "synonym_file = open(synonym_path, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [line for line in w2v_file.readlines()]\n",
    "antonym_lines = [line for line in antonym_file.readlines()]\n",
    "synonym_lines = [line for line in synonym_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_words = words[0]\n",
    "vector_dimensions = words[1]\n",
    "words = words[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_words = []\n",
    "embedding = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, w in enumerate(words):\n",
    "    arr_words = words[idx].split(' ')\n",
    "    word = arr_words[0]\n",
    "    weight = []\n",
    "    for i in arr_words[2:len(arr_words)-1]:\n",
    "        weight.append(float(i))\n",
    "    embedding[word] = np.array(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "antonym_word1 = []\n",
    "antonym_word2 = []\n",
    "synonym_word1 = []\n",
    "synonym_word2 = []\n",
    "\n",
    "for idx, w in enumerate(antonym_lines):\n",
    "    arr_words = antonym_lines[idx].split(' ')\n",
    "    antonym_word1.append(arr_words[0])\n",
    "    antonym_word2.append(arr_words[1].split('\\n')[0])\n",
    "\n",
    "for idx, w in enumerate(synonym_lines):\n",
    "    arr_words = synonym_lines[idx].split(' ')\n",
    "    synonym_word1.append(arr_words[0])\n",
    "    synonym_word2.append(arr_words[1].split('\\n')[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_classification(n_samples=100, random_state=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "                                                     random_state=1)\n",
    "clf = MLPClassifier(random_state=1, max_iter=20).fit(X_train, y_train)\n",
    "clf.predict_proba(X_test[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([])\n",
    "b = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(antonym_word1)):\n",
    "    if antonym_word1[i] in embedding and antonym_word2[i] in embedding:\n",
    "        if len(a) == 0:\n",
    "            a = np.append(a, embedding[antonym_word1[i]])\n",
    "            b = np.append(b, embedding[antonym_word2[i]])\n",
    "        else:\n",
    "            a = np.vstack((a, embedding[antonym_word1[i]]))\n",
    "            b = np.vstack((b, embedding[antonym_word2[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([])\n",
    "b1 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(synonym_word1)):\n",
    "    if synonym_word1[i] in embedding and synonym_word2[i] in embedding:\n",
    "        if len(a1) == 0:\n",
    "            a1 = np.append(a1, embedding[synonym_word1[i]])\n",
    "            b1 = np.append(b1, embedding[synonym_word2[i]])\n",
    "        else:\n",
    "            a1 = np.vstack((a1, embedding[synonym_word1[i]]))\n",
    "            b1 = np.vstack((b1, embedding[synonym_word2[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_syn = np.ones(len(a1))\n",
    "y_ant = np.zeros(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((y_syn, y_ant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ant_train = np.concatenate((a, b), axis=1)\n",
    "syn_train = np.concatenate((a1, b1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.concatenate((syn_train, ant_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clf = MLPClassifier(random_state=1, max_iter=200).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_path = \"source/ViData/ViCon/400_noun_pairs.txt\"\n",
    "adj_path = \"source/ViData/ViCon/600_adj_pairs.txt\"\n",
    "verb_path = \"source/ViData/ViCon/400_verb_pairs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_file = open(noun_path, encoding='utf8')\n",
    "adj_file = open(adj_path, encoding='utf8')\n",
    "verb_file = open(verb_path, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_line_test = [line for line in noun_file.readlines()]\n",
    "verb_line_test = [line for line in verb_file.readlines()]\n",
    "adj_line_test = [line for line in adj_file.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_line_test = noun_line_test[1:]\n",
    "verb_line_test = verb_line_test[1:]\n",
    "adj_line_test = adj_line_test[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test_1 = []\n",
    "words_test_2 = []\n",
    "label = []\n",
    "\n",
    "for idx, w in enumerate(noun_line_test):\n",
    "    words_test_1.append(noun_line_test[idx].split('\\t')[0])\n",
    "    words_test_2.append(noun_line_test[idx].split('\\t')[1])\n",
    "    if noun_line_test[idx].split('\\t')[2].split('\\n')[0] == 'ANT':\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "        \n",
    "for idx, w in enumerate(verb_line_test):\n",
    "    words_test_1.append(verb_line_test[idx].split('\\t')[0])\n",
    "    words_test_2.append(verb_line_test[idx].split('\\t')[1])\n",
    "    if verb_line_test[idx].split('\\t')[2].split('\\n')[0] == 'ANT':\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)\n",
    "        \n",
    "for idx, w in enumerate(adj_line_test):\n",
    "    words_test_1.append(adj_line_test[idx].split('\\t')[0])\n",
    "    words_test_2.append(adj_line_test[idx].split('\\t')[1])\n",
    "    if adj_line_test[idx].split('\\t')[2].split('\\n')[0] == 'ANT':\n",
    "        label.append(0)\n",
    "    else:\n",
    "        label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_test_vt1 = np.array([])\n",
    "words_test_vt2 = np.array([])\n",
    "real_test_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words_test_1)):\n",
    "    if words_test_1[i] in embedding and words_test_2[i] in embedding:\n",
    "        if len(words_test_vt1) == 0:\n",
    "            words_test_vt1 = np.append(words_test_vt1, embedding[words_test_1[i]])\n",
    "            words_test_vt2 = np.append(words_test_vt2, embedding[words_test_2[i]])\n",
    "            real_test_label.append(label[i])\n",
    "        else:\n",
    "            words_test_vt1 = np.vstack((words_test_vt1, embedding[words_test_1[i]]))\n",
    "            words_test_vt2 = np.vstack((words_test_vt2, embedding[words_test_2[i]]))\n",
    "            real_test_label.append(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.concatenate((words_test_vt1, words_test_vt2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_ = np.array(real_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LG = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr').fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg_pred = LG.predict(test_data[:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for i in range(len(lg_pred)):\n",
    "    if lg_pred[i] == label_[i]:\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6743393009377664\n"
     ]
    }
   ],
   "source": [
    "print(c/len(lg_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = 0\n",
    "false_positive = 0\n",
    "false_negative = 0\n",
    "for i in range(len(lg_pred)):\n",
    "    if lg_pred[i] == label_[i]:\n",
    "        if lg_pred[i] == 1:\n",
    "            true_positive += 1\n",
    "    else:\n",
    "        if label_[i] == 1:\n",
    "            false_positive += 1\n",
    "        else:\n",
    "            false_negative += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = (true_positive) / (true_positive + false_positive)\n",
    "recall = true_positive / (true_positive + false_negative)\n",
    "F1 = 2 * ((precision * recall) / (precision + recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9194756554307116\n",
      "0.591566265060241\n",
      "0.7199413489736071\n"
     ]
    }
   ],
   "source": [
    "print(precision)\n",
    "print(recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "visim_path = \"source/ViData/ViSim-400/Visim-400.txt\"\n",
    "visim_file = open(visim_path, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "visim_line = [line for line in visim_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "visim_line = visim_line[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "visim_word1 = []\n",
    "visim_word2 = []\n",
    "\n",
    "for line in visim_line:\n",
    "    visim_word1.append(line.split('\\t')[0])\n",
    "    visim_word2.append(line.split('\\t')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(A, B):\n",
    "   return np.sum(A*B) / np.sqrt(np.sum(A**2)*np.sum(B**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_list = []\n",
    "for i in range(len(visim_word1)):\n",
    "    if visim_word1[i] in embedding and visim_word2[i] in embedding:\n",
    "        cosine_similarity_list.append(cosine_similarity(embedding[visim_word1[i]], embedding[visim_word2[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0825231832921177"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(w1, w2):\n",
    "    x = np.concatenate((embedding[w1], embedding[w2]), axis=0)\n",
    "    return LG.predict(np.reshape(x, (1,300)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_data('thảnh_thơi', 'ưu_tư')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(X)\n",
    "kmeans.labels_\n",
    "kmeans.predict([[0, 0], [12, 3], [0,10]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = lis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
